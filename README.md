# ğŸ§  rag-deep-dive
A step-by-step, from-scratch implementation of a Retrieval-Augmented Generation (RAG) system â€” built for learning, experimentation, and understanding every internal component deeply.

This project does **not** rely on heavy frameworks like LangChain or LlamaIndex.  
Everything is built manually so you can see how RAG works end-to-end:

Text â†’ Chunker â†’ Embedder â†’ Vector Store â†’ Retriever â†’ LLM â†’ Answer

---

## ğŸš€ Features (Current State)

### âœ”ï¸ **1. Document Chunking**
- Splits large documents into clean, paragraph-aware chunks.
- Max token/char limit per chunk.
- Produces ~800â€“character chunks.

**File:** `chunker.py`

---

### âœ”ï¸ **2. Embedding Generation**
- Uses a local or small embedding model (OpenAI-compatible API or similar).
- Converts each chunk into a **1024-dimensional embedding vector**.
- Handles:
  - batching
  - normalization
  - embedding persistence

**File:** `embedder.py`

---

### âœ”ï¸ **3. Simple Vector Store**
- Stores embeddings + chunks in memory.
- Saves and loads them using `pickle`.
- Supports cosine similarity search using NumPy.

**File:** `vector_store.py`

---

### âœ”ï¸ **4. Retriever**
- Encodes user queries using the same embedding model.
- Computes similarity with stored embeddings.
- Returns **top-k chunks**.

**File:** `retrieve.py`

---

### âœ”ï¸ **5. Chat Loop**
- Accepts user input from CLI.
- Retrieves context.
- Sends context + query to an LLM (OpenAI/Local model).
- Generates an answer.

**File:** `main.py`

---

## ğŸ“ Project Structure

```plaintext
ğŸ“¦ rag-deep-dive  
â”œâ”€â”€ ğŸ“„ main.py  
â”œâ”€â”€ ğŸ“„ chunker.py  
â”œâ”€â”€ ğŸ“„ embedder.py  
â”œâ”€â”€ ğŸ“„ vector_store.py  
â”œâ”€â”€ ğŸ“„ retrieve.py  
â”‚
â”œâ”€â”€ ğŸ“‚ data  
â”‚   â””â”€â”€ ğŸ“„ your_docs.txt  
â”‚
â”œâ”€â”€ ğŸ“‚ index  
â”‚   â””â”€â”€ ğŸ—‚ï¸ index.pkl  
â”‚
â””â”€â”€ ğŸ“„ README.md
```



---

## ğŸ§° Requirements
    python >= 3.10
    numpy
    openai
    tqdm

Create a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```


## â–¶ï¸ Running the System

1. Build the index (chunk + embed + store)

    ```bash 
    python main.py --build --file data/your_docs.txt
    ```


2. Run chat mode

    ```bash 
    python main.py --chat
    ```



## ğŸ’¾ Vector Index

Embeddings and metadata are saved here:

```index/index.pkl```

Contains:
- list of chunk texts
- embeddings matrix (N Ã— 1024)
- metadata

This allows you to load the index instantly without recomputing embeddings.


## ğŸ” Current Limitations (before next commit)
- No advanced debugging tools yet.
- No visualization of similarity or vectors.
- No Chroma or FAISSâ€”using a simple in-memory store for learning.
- Pipeline is intentionally simple for step-by-step understanding.



## ğŸ› ï¸ Next Planned Steps (future commits)

You will add:
    â€¢	Debug logs at each step
    (chunk sizes, embedding shapes, similarity scores)
    â€¢	PCA/2D visualization of embeddings
    â€¢	Inspect each retrieved chunk
    â€¢	Optionally swap in:
    â€¢	FAISS
    â€¢	ChromaDB
    â€¢	better embedding models

These will be added as new commits and branches.



## ğŸ“œ License

MIT License â€” free to use and modify.


## â­ Motivation

This project is designed to help developers truly understand how RAG works under the hood:
	â€¢	How embeddings are generated
	â€¢	How similarity search operates
	â€¢	How chunking affects retrieval
	â€¢	How LLMs combine retrieved context with queries

Instead of treating RAG as a black box, this repo reveals every piece step-by-step.